{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889},{"sourceId":1187790,"sourceType":"datasetVersion","datasetId":675484}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Project Title: Face Mask Detection using TensorFlow**","metadata":{}},{"cell_type":"markdown","source":"**Dataset link:-**\n/kaggle/input/face-mask-detection\n\n/kaggle/input/face-mask-12k-images-dataset","metadata":{}},{"cell_type":"markdown","source":"# **Objective:**\n**To build a deep learning model that automatically detects whether people in images are wearing face masks or not using object detection techniques.**\n\n# **Dataset Description:**\n\n**Images of people in public places with/without face masksAnnotations provided in Pascal VOC XML format, or converted to CSV format with:filename, xmin, ymin, xmax, ymax, classLabels:with_mask (or class 0)without_mask (or class 1)**\n\n\n\n# **Workflow Steps:**\n**Setup Environment**\n\nInstall TensorFlow Object Detection API and dependencies\n\nConfigure directory structure\n\n**Prepare Data**\n\nLoad dataset and convert annotations (XML/CSV â†’ TFRecord)\n\nCreate label map (label_map.pbtxt)\n\n**Model Selection**\n\nChoose a pre-trained model\n\nModify pipeline.config for the dataset\n\n**Train the Model**\n\nTrain using model_main_tf2.py\n\nSave checkpoints, logs\n\n**Evaluate the Model**\n\nVisualize loss and accuracy using TensorBoard\n\nExport trained model\n\n**Run Inference**\n\nLoad the trained model\n\nPredict on test images\n\nVisualize bounding boxes with predicted classes and confidence","metadata":{}},{"cell_type":"markdown","source":"# **Cell 1: Library Imports**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport cv2\nimport seaborn as sns\nimport PIL\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:02:16.575493Z","iopub.execute_input":"2025-06-27T16:02:16.575746Z","iopub.status.idle":"2025-06-27T16:02:52.525086Z","shell.execute_reply.started":"2025-06-27T16:02:16.575724Z","shell.execute_reply":"2025-06-27T16:02:52.524007Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 2: Preparing directories**\n","metadata":{}},{"cell_type":"code","source":"# Path\nimg_dir = '/kaggle/input/face-mask-detection/images'\n\"\"\"\nannotation_directory contains files, and each file is associated to only one image,\nand it contains the height and width of the image and also xmin,ymin,xmax, and ymax of each boundary box\ninside the image\n\"\"\"\nannotation_dir = '/kaggle/input/face-mask-detection/annotations'\n\ninput_dir = '/kaggle/input/face-mask-detection'\noutput_dir = '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:03:32.346673Z","iopub.execute_input":"2025-06-27T16:03:32.346999Z","iopub.status.idle":"2025-06-27T16:03:32.351859Z","shell.execute_reply.started":"2025-06-27T16:03:32.346966Z","shell.execute_reply":"2025-06-27T16:03:32.350628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 3: Viewing some images**\n\n","metadata":{}},{"cell_type":"code","source":"# sample visuaization\nfor idx, image in enumerate(os.listdir(img_dir)):\n    img = cv2.imread(os.path.join(img_dir, image), 1)\n    plt.imshow(img)\n    plt.show()\n    \n    if idx == 3:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:04:40.902050Z","iopub.execute_input":"2025-06-27T16:04:40.903213Z","iopub.status.idle":"2025-06-27T16:04:42.091767Z","shell.execute_reply.started":"2025-06-27T16:04:40.903172Z","shell.execute_reply":"2025-06-27T16:04:42.090746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport pandas as pd\nimport os\n\ndef parse_annotation(xml_file):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n\n    boxes = []\n    filename = root.find('filename').text\n\n    for obj in root.findall('object'):\n        label = obj.find('name').text\n        bbox = obj.find('bndbox')\n        xmin = int(bbox.find('xmin').text)\n        ymin = int(bbox.find('ymin').text)\n        xmax = int(bbox.find('xmax').text)\n        ymax = int(bbox.find('ymax').text)\n        boxes.append([filename, xmin, ymin, xmax, ymax, label])\n\n    return boxes\n\nannotation_list = []\n\nfor xml_file in os.listdir(annotation_dir):\n    if xml_file.endswith('.xml'):\n        xml_path = os.path.join(annotation_dir, xml_file)\n        annotation_list.extend(parse_annotation(xml_path))\n\ndf = pd.DataFrame(annotation_list, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"label\"])\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T15:52:42.211082Z","iopub.execute_input":"2025-06-30T15:52:42.211658Z","iopub.status.idle":"2025-06-30T15:52:42.919783Z","shell.execute_reply.started":"2025-06-30T15:52:42.211633Z","shell.execute_reply":"2025-06-30T15:52:42.918836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 4: Storing images paths**\n","metadata":{}},{"cell_type":"code","source":"img_file_path = []\nfor img in os.listdir(img_dir): # img here is the name of the image not the image itself\n    image = cv2.imread(os.path.join(img_dir,img), 0) # 0 for grayscale\n    img_file_path.append(f'{img}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:05:29.475917Z","iopub.execute_input":"2025-06-27T16:05:29.476524Z","iopub.status.idle":"2025-06-27T16:05:49.898271Z","shell.execute_reply.started":"2025-06-27T16:05:29.476495Z","shell.execute_reply":"2025-06-27T16:05:49.897183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 5: Libraries needed for reading XML files**\n\n\n","metadata":{}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET \n\"\"\"\nElementTree module provides \na way to work with Extensible Markup Language (XML) documents as a tree-like structure of elements.\n\"\"\"\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:06:26.101479Z","iopub.execute_input":"2025-06-27T16:06:26.101862Z","iopub.status.idle":"2025-06-27T16:06:26.105829Z","shell.execute_reply.started":"2025-06-27T16:06:26.101826Z","shell.execute_reply":"2025-06-27T16:06:26.104985Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 6: Reading XML files**","metadata":{}},{"cell_type":"code","source":"df = { 'name': [],\n        'label': [],\n      'width': [],\n      'height': [],\n     'xmin': [],\n     'ymin': [],\n     'xmax': [],\n     'ymax': []}\n\"\"\"\nThe glob.glob() function returns a list of all the pathnames that match the specified pattern.\nThe resulting list can then be used to process or analyze the files that match the pattern.\n\"\"\"\nfor idx, anno in enumerate(glob.glob(annotation_dir + '/*.xml')):\n    trees = ET.parse(anno)\n    \n    #print(anno) print/view the annotation to understand the following code\n    root = trees.getroot()\n    width, height = [], []\n    for item in root.iter():\n        if item.tag == 'size':\n            for attr in list(item):\n                if attr.tag == 'width':\n                    width = int(round(float(attr.text)))\n                if attr.tag == 'height':\n                    height = int(round(float(attr.text)))\n                    \n        if item.tag == 'object':\n            for attr in list(item):\n                if 'name' in attr.tag:\n                    label = attr.text\n                    df['label'] += [label]\n                    df['width'] += [width]\n                    df['height'] += [height]\n                    #dataset['name']+=[anno.split('/')[-1][0:-4]] \n                    df['name'] += [anno.split('/')[-1][0:-4]]\n                    \n                if 'bndbox' in attr.tag:\n                    for dim in attr:\n                        if dim.tag == 'xmin':\n                            xmin = int(round(float(dim.text)))\n                            df['xmin'] += [xmin]\n                            \n                        if dim.tag == 'ymin':\n                            ymin = int(round(float(dim.text)))\n                            df['ymin'] += [ymin]\n                        if dim.tag == 'xmax':\n                            xmax = int(round(float(dim.text)))\n                            df['xmax'] += [xmax]\n                        if dim.tag == 'ymax':\n                            ymax = int(round(float(dim.text)))\n                            df['ymax'] += [ymax]\n                    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:08:09.432132Z","iopub.execute_input":"2025-06-27T16:08:09.432434Z","iopub.status.idle":"2025-06-27T16:08:12.969435Z","shell.execute_reply.started":"2025-06-27T16:08:09.432411Z","shell.execute_reply":"2025-06-27T16:08:12.968578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 7: Viewing data frame of images**\n","metadata":{}},{"cell_type":"code","source":"df1 = pd.DataFrame(df)\ndf1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:09:01.882010Z","iopub.execute_input":"2025-06-27T16:09:01.882897Z","iopub.status.idle":"2025-06-27T16:09:01.898457Z","shell.execute_reply.started":"2025-06-27T16:09:01.882868Z","shell.execute_reply":"2025-06-27T16:09:01.897726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 8: Describing data frame**","metadata":{}},{"cell_type":"code","source":"df1.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:09:51.616169Z","iopub.execute_input":"2025-06-27T16:09:51.616447Z","iopub.status.idle":"2025-06-27T16:09:51.642683Z","shell.execute_reply.started":"2025-06-27T16:09:51.616429Z","shell.execute_reply":"2025-06-27T16:09:51.641930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 9: Maping labels to integers**\n","metadata":{}},{"cell_type":"code","source":"label_map = { 'without_mask': 0,\n            'with_mask': 1,\n            'mask_weared_incorrect': 2}\n\ndf1['class'] = df1['label'].map(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:10:38.826298Z","iopub.execute_input":"2025-06-27T16:10:38.826591Z","iopub.status.idle":"2025-06-27T16:10:38.832932Z","shell.execute_reply.started":"2025-06-27T16:10:38.826569Z","shell.execute_reply":"2025-06-27T16:10:38.832162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:10:54.656315Z","iopub.execute_input":"2025-06-27T16:10:54.656698Z","iopub.status.idle":"2025-06-27T16:10:54.666534Z","shell.execute_reply.started":"2025-06-27T16:10:54.656635Z","shell.execute_reply":"2025-06-27T16:10:54.665714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 10: Splitting the data**","metadata":{}},{"cell_type":"code","source":"# split train, test, val data\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(img_file_path, test_size=0.2, random_state=101)\ntrain, val = train_test_split(train, test_size=0.15, random_state=101)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:12:03.640736Z","iopub.execute_input":"2025-06-27T16:12:03.641049Z","iopub.status.idle":"2025-06-27T16:12:03.647221Z","shell.execute_reply.started":"2025-06-27T16:12:03.641025Z","shell.execute_reply":"2025-06-27T16:12:03.646361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 11: Preparing yolo v5 model.**","metadata":{}},{"cell_type":"code","source":"# yolo v5\n!git clone https://github.com/ultralytics/yolov5\n%cd yolov5\n!pip install -qr requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:15:02.346493Z","iopub.execute_input":"2025-06-27T16:15:02.346834Z","iopub.status.idle":"2025-06-27T16:15:06.887940Z","shell.execute_reply.started":"2025-06-27T16:15:02.346810Z","shell.execute_reply":"2025-06-27T16:15:06.886726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 12: Preparing some new directories.**","metadata":{}},{"cell_type":"code","source":"\nimport os\n\n# Change the working directory\nos.chdir('/kaggle/working')\n\n# Define base path for YOLOv5 data\nbase_path = './yolov5/data'\n\n# List of subdirectories to create\nsubdirs = [\n    'train/images',\n    'train/labels',\n    'val/images',\n    'val/labels',\n    'test/images',\n    'test/labels'\n]\n\n# Create each subdirectory\nfor subdir in subdirs:\n    full_path = os.path.join(base_path, subdir)\n    os.makedirs(full_path, exist_ok=True)  # Avoids FileExistsError\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:17:12.736976Z","iopub.execute_input":"2025-06-27T16:17:12.738013Z","iopub.status.idle":"2025-06-27T16:17:12.743387Z","shell.execute_reply.started":"2025-06-27T16:17:12.737981Z","shell.execute_reply":"2025-06-27T16:17:12.742641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 13: Copying the image data in the yolov5 folder**","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\n# Correct path to the images\nimg_dir = '/kaggle/input/face-mask-detection/images'\noutput_dir = '/kaggle/working'\n\ndef open_image_file(image_items, folder_name):\n    for image in image_items:\n        try:\n            img_path = os.path.join(img_dir, image)\n            img = Image.open(img_path)\n            img1 = img.resize((640, 480))\n            save_path = os.path.join(output_dir, f'yolov5/data/{folder_name}/images/{image}')\n            img1.save(save_path)\n        except Exception as e:\n            print(f\"Error processing {image}: {e}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:24:20.162591Z","iopub.execute_input":"2025-06-27T16:24:20.162947Z","iopub.status.idle":"2025-06-27T16:24:20.168575Z","shell.execute_reply.started":"2025-06-27T16:24:20.162923Z","shell.execute_reply":"2025-06-27T16:24:20.167857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 14: Resizing boxes to match with the new images size**","metadata":{}},{"cell_type":"code","source":"df1['xmin'] = (640/df1['width']) * df1['xmin']\ndf1['ymin'] = (480/df1['height']) * df1['ymin']\ndf1['xmax'] = (640/df1['width']) * df1['xmax']\ndf1['ymax'] = (480/df1['height']) * df1['ymax']\ndf1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:24:26.972612Z","iopub.execute_input":"2025-06-27T16:24:26.972984Z","iopub.status.idle":"2025-06-27T16:24:27.004070Z","shell.execute_reply.started":"2025-06-27T16:24:26.972960Z","shell.execute_reply":"2025-06-27T16:24:27.002826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1[['xmin', 'ymin', 'xmax', 'ymax']] = df1[['xmin', 'ymin', 'xmax', 'ymax']].astype('int')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:25:07.998219Z","iopub.execute_input":"2025-06-27T16:25:07.999222Z","iopub.status.idle":"2025-06-27T16:25:08.010227Z","shell.execute_reply.started":"2025-06-27T16:25:07.999190Z","shell.execute_reply":"2025-06-27T16:25:08.009409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WIDTH = 640\nHEIGHT = 480","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:25:18.807680Z","iopub.execute_input":"2025-06-27T16:25:18.808003Z","iopub.status.idle":"2025-06-27T16:25:18.812145Z","shell.execute_reply.started":"2025-06-27T16:25:18.807979Z","shell.execute_reply":"2025-06-27T16:25:18.811296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 15: Converting from left upper corner and right bottom corner of boxes to just midpoint, height, and width of boxes**","metadata":{}},{"cell_type":"code","source":"df1['x_center'] = (df1['xmin']+df1['xmax'])/(2*WIDTH)\ndf1['y_center'] = (df1['ymin']+df1['ymax'])/(2*HEIGHT)\ndf1['box_width'] = (df1['xmax']-df1['xmin'])/ WIDTH\ndf1['box_height'] = (df1['ymax']-df1['ymin'])/ HEIGHT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:25:52.482311Z","iopub.execute_input":"2025-06-27T16:25:52.482641Z","iopub.status.idle":"2025-06-27T16:25:52.492984Z","shell.execute_reply.started":"2025-06-27T16:25:52.482616Z","shell.execute_reply":"2025-06-27T16:25:52.492070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:26:01.957493Z","iopub.execute_input":"2025-06-27T16:26:01.957845Z","iopub.status.idle":"2025-06-27T16:26:01.971103Z","shell.execute_reply.started":"2025-06-27T16:26:01.957822Z","shell.execute_reply":"2025-06-27T16:26:01.970337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1 = df1.astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:26:13.582475Z","iopub.execute_input":"2025-06-27T16:26:13.583381Z","iopub.status.idle":"2025-06-27T16:26:13.612742Z","shell.execute_reply.started":"2025-06-27T16:26:13.583351Z","shell.execute_reply":"2025-06-27T16:26:13.611770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 16: Viewing some images paths**","metadata":{}},{"cell_type":"code","source":"img_file_path[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:26:54.943549Z","iopub.execute_input":"2025-06-27T16:26:54.944344Z","iopub.status.idle":"2025-06-27T16:26:54.949692Z","shell.execute_reply.started":"2025-06-27T16:26:54.944315Z","shell.execute_reply":"2025-06-27T16:26:54.948718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 17: Writing info for each images**","metadata":{}},{"cell_type":"code","source":"def copy_label(label_items, folder_name):\n    file_name = [x.split('.')[0] for x in img_file_path]\n    for name in file_name:\n        data = df1[df1.name == name]\n        box_list = []\n        for idx in range(len(data)):\n            row = data.iloc[idx]\n            box_list.append(row['class']+\" \"+row['x_center']+\" \"+row['y_center']+\" \"+ row['box_width']+\" \"+row['box_height'])\n\n        text = \"\\n\".join(box_list)\n        with open(f'{output_dir}/yolov5/data/{folder_name}/labels/{name}.txt', 'w') as file:\n            file.write(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:27:38.524611Z","iopub.execute_input":"2025-06-27T16:27:38.525525Z","iopub.status.idle":"2025-06-27T16:27:38.531253Z","shell.execute_reply.started":"2025-06-27T16:27:38.525496Z","shell.execute_reply":"2025-06-27T16:27:38.530343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"copy_label(train, 'train')\ncopy_label(val, 'val')\ncopy_label(test, 'test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:27:49.478943Z","iopub.execute_input":"2025-06-27T16:27:49.479510Z","iopub.status.idle":"2025-06-27T16:27:52.566258Z","shell.execute_reply.started":"2025-06-27T16:27:49.479480Z","shell.execute_reply":"2025-06-27T16:27:52.565175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.chdir('/kaggle/working/yolov5/data/train/labels')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:28:09.224757Z","iopub.execute_input":"2025-06-27T16:28:09.225041Z","iopub.status.idle":"2025-06-27T16:28:09.229201Z","shell.execute_reply.started":"2025-06-27T16:28:09.225022Z","shell.execute_reply":"2025-06-27T16:28:09.228324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat maksssksksss0.txt ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:28:12.344260Z","iopub.execute_input":"2025-06-27T16:28:12.344602Z","iopub.status.idle":"2025-06-27T16:28:12.485589Z","shell.execute_reply.started":"2025-06-27T16:28:12.344576Z","shell.execute_reply":"2025-06-27T16:28:12.484467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 18: Creating yaml file**","metadata":{}},{"cell_type":"code","source":"# Configure .yaml file \nyaml_file = \"\"\"train: /kaggle/working/yolov5/data/train/images\nval: /kaggle/working/yolov5/data/val/images\n                \nnc: 3\nnames: [without_mask, with_mask, mask_weared_incorrect]\"\"\"\n\nwith open('/kaggle/working/yolov5/data/data.yaml', 'w') as f:\n    f.write(yaml_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:28:52.290070Z","iopub.execute_input":"2025-06-27T16:28:52.290451Z","iopub.status.idle":"2025-06-27T16:28:52.296120Z","shell.execute_reply.started":"2025-06-27T16:28:52.290419Z","shell.execute_reply":"2025-06-27T16:28:52.295171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cat /kaggle/working/yolov5/data/data.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:29:04.489805Z","iopub.execute_input":"2025-06-27T16:29:04.490183Z","iopub.status.idle":"2025-06-27T16:29:04.627241Z","shell.execute_reply.started":"2025-06-27T16:29:04.490158Z","shell.execute_reply":"2025-06-27T16:29:04.626037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 19: Training**","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/yolov5/train.py --img 640 --epochs 100 --batch 32 --data /kaggle/working/yolov5/data/data.yaml  --weights yolov5s.pt  --cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:29:52.229611Z","iopub.execute_input":"2025-06-27T16:29:52.229986Z","iopub.status.idle":"2025-06-27T16:33:19.876538Z","shell.execute_reply.started":"2025-06-27T16:29:52.229955Z","shell.execute_reply":"2025-06-27T16:33:19.875192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 20: Displaying performance of model**","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image as Display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:33:40.458005Z","iopub.execute_input":"2025-06-27T16:33:40.459058Z","iopub.status.idle":"2025-06-27T16:33:40.463116Z","shell.execute_reply.started":"2025-06-27T16:33:40.459022Z","shell.execute_reply":"2025-06-27T16:33:40.462136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display, Image\n\nimg_path = '/kaggle/working/yolov5/runs/train/exp/labels.jpg'\ndisplay(Image(filename=img_path, width=1080))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:35:45.365416Z","iopub.execute_input":"2025-06-27T16:35:45.365774Z","iopub.status.idle":"2025-06-27T16:35:45.377029Z","shell.execute_reply.started":"2025-06-27T16:35:45.365749Z","shell.execute_reply":"2025-06-27T16:35:45.376129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = '/kaggle/working/yolov5/runs/train/exp/labels_correlogram.jpg'\ndisplay(Image(filename=img_path, width=1080))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:36:13.665280Z","iopub.execute_input":"2025-06-27T16:36:13.665608Z","iopub.status.idle":"2025-06-27T16:36:13.678238Z","shell.execute_reply.started":"2025-06-27T16:36:13.665583Z","shell.execute_reply":"2025-06-27T16:36:13.677242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Cell 21: Displaying Training and Validation batches**","metadata":{}},{"cell_type":"code","source":"Display(filename='/kaggle/working/yolov5/runs/train/exp/train_batch0.jpg', width=600)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:36:19.884918Z","iopub.execute_input":"2025-06-27T16:36:19.885252Z","iopub.status.idle":"2025-06-27T16:36:19.904877Z","shell.execute_reply.started":"2025-06-27T16:36:19.885227Z","shell.execute_reply":"2025-06-27T16:36:19.903860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import display\n\nimg = Image.open(\"/kaggle/working/yolov5/runs/train/exp/train_batch0.jpg\")\ndisplay(img)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:37:06.945714Z","iopub.execute_input":"2025-06-27T16:37:06.946542Z","iopub.status.idle":"2025-06-27T16:37:07.542137Z","shell.execute_reply.started":"2025-06-27T16:37:06.946513Z","shell.execute_reply":"2025-06-27T16:37:07.540570Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Final Summary**\n***This project implements an end-to-end Face Mask Detection system using the TensorFlow Object Detection API. The dataset comprises real-world images of people in public with and without face masks. After preparing annotations and converting them into TFRecords, a pre-trained model was fine-tuned to detect masks on faces with high accuracy. The final model was capable of detecting multiple faces and classifying them as \"with mask\", \"without mask\", or optionally \"mask worn incorrectly\". The system can help enforce public safety compliance and can be integrated into surveillance systems or crowd-monitoring tools.***","metadata":{}}]}